import os
import time
import json
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from typing import List, Tuple, Dict
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI
from dotenv import load_dotenv

# ----------------------------------------
# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
# ----------------------------------------
load_dotenv()

def get_openai_api_key() -> str:
    """
    OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì½ì–´ì„œ ê³µë°± ì œê±° í›„ ë¦¬í„´.
    (ë¡œì»¬ .env / GitHub Actions env ë‘˜ ë‹¤ ì—¬ê¸°ë¡œ ë“¤ì–´ì˜´)
    """
    key = os.getenv("OPENAI_API_KEY", "")
    return key.strip()

OPENAI_API_KEY = get_openai_api_key()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").strip()

if not OPENAI_API_KEY:
    # ì—¬ê¸°ì„œ ë°”ë¡œ ì£½ì—¬ë²„ë¦¬ë©´, GitHub Actions ë¡œê·¸ì—ì„œ ì›ì¸ì„ ë°”ë¡œ ì•Œ ìˆ˜ ìˆìŒ
    raise SystemExit(
        "[ERROR] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        " - ë¡œì»¬: .env íŒŒì¼ì— OPENAI_API_KEY=... ì¶”ê°€\n"
        " - GitHub Actions: workflow ymlì—ì„œ env: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} ë¡œ ì „ë‹¬ í•„ìš”"
    )

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# ì‚¬ìš©í•  GPT ëª¨ë¸ (ì›í•˜ë©´ í™˜ê²½ë³€ìˆ˜ë¡œ ë¹¼ë„ ë¨)
GPT_MODEL_NAME = os.getenv("GPT_MODEL_NAME", "gpt-4.1-mini").strip()


PRESS_LIST: List[Tuple[str, str]] = [
    ("ë™ì•„ì¼ë³´", "020"),
    ("í•œêµ­ì¼ë³´", "469"),
    ("ì¡°ì„ ì¼ë³´", "023"),
    ("ì¤‘ì•™ì¼ë³´", "025"),
    ("í•œê²¨ë ˆ", "028"),
    ("ê²½í–¥ì‹ ë¬¸", "032"),
]

# ----------------------------------------
# [Part 1] ë„¤ì´ë²„ 1ë©´ ë§í¬ ìˆ˜ì§‘
# ----------------------------------------
def get_kst_today() -> str:
    now_utc = datetime.now(timezone.utc)
    now_kst = now_utc + timedelta(hours=9)
    return now_kst.strftime("%Y%m%d")

def fetch_html(url: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    resp = requests.get(url.strip(), headers=headers, timeout=20)
    resp.raise_for_status()
    return resp.text

def extract_a1_links(html: str, page_url: str, press_code: str, date: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates: List[str] = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if f"/article/newspaper/{press_code}/" not in href:
            continue
        if f"date={date}" not in href:
            continue
        full_url = urljoin(page_url, href)

        is_a1 = False
        parent = a
        for _ in range(6):
            parent = parent.parent
            if parent is None:
                break
            text = parent.get_text(" ", strip=True)
            if any(key in text for key in ["A1ë©´", "A01ë©´", "1ë©´", "1 é¢"]):
                is_a1 = True
                break
        if is_a1:
            candidates.append(full_url)

    if not candidates:  # Fallback
        seen = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if f"/article/newspaper/{press_code}/" in href and f"date={date}" in href:
                full_url = urljoin(page_url, href)
                if full_url not in seen:
                    candidates.append(full_url)
                    seen.add(full_url)
            if len(candidates) >= 4:
                break
    return list(set(candidates))

def collect_naver_news_links() -> List[Dict[str, str]]:
    date = get_kst_today()
    print(f"[INFO] {date}ì¼ì 1ë©´ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘")
    all_items = []
    for press_name, press_code in PRESS_LIST:
        url = ""
        try:
            url = f"https://media.naver.com/press/{press_code}/newspaper?date={date}".strip()
            html = fetch_html(url)
            links = extract_a1_links(html, url, press_code, date)
            for link in links:
                all_items.append({"source": press_name, "url": link})
        except Exception as e:
            print(f"  [ì—ëŸ¬] {press_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            print(f"  [URL] ìš”ì²­ ì‹¤íŒ¨ URL: {url}")
    return all_items

# ----------------------------------------
# [Part 2] ë³¸ë¬¸ í¬ë¡¤ë§
# ----------------------------------------
def fetch_single_article_content(item: dict) -> dict:
    try:
        resp = requests.get(item["url"], headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        selectors = ["div#dic_area", "div#newsEndContents", "div.newsct_article", "div#articleBodyContents"]
        content = ""
        for selector in selectors:
            node = soup.select_one(selector)
            if node:
                content = node.get_text("\n", strip=True)
                break
        return {
            "source": item["source"],
            "url": item["url"],
            "content": content[:4000] if content else "ë³¸ë¬¸ ì—†ìŒ"
        }
    except Exception:
        return item

def fetch_contents_parallel(items: list) -> list:
    print(f"[INFO] ì´ {len(items)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_single_article_content, items))
    return results

# ----------------------------------------
# [Part 3] GPT ë¶„ì„ (ë¦¬í¬íŠ¸ ì‘ì„±)
# ----------------------------------------
def analyze_with_gpt(articles: list) -> dict:
    if client is None:
        print("[CRITICAL ERROR] OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return {"topics": []}

    print(f"[INFO] {GPT_MODEL_NAME} ë¶„ì„ ìš”ì²­ ì‹œì‘...")

    articles_text = ""
    for i, art in enumerate(articles):
        articles_text += f"[ID:{i}] ì–¸ë¡ ì‚¬:{art['source']} | ë‚´ìš©:{art['content'][:2000]}\n"

    # í”„ë¡¬í”„íŠ¸ì—ì„œ JSON í˜•ì‹ ê°•í•˜ê²Œ ìš”êµ¬
    prompt = f"""
    ë„ˆëŠ” ì „ë¬¸ ë‰´ìŠ¤ ì—ë””í„°ë‹¤. ì˜¤ëŠ˜ì ì‹ ë¬¸ 1ë©´ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ê³ í’ˆì§ˆ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.

    [ìš”êµ¬ì‚¬í•­]
    1. ê¸°ì‚¬ë“¤ì„ ìœ ì‚¬í•œ ì£¼ì œ(ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“±)ë¡œ ê·¸ë£¹í™”í•˜ë¼.
    2. **ì£¼ì œë³„ í†µí•© ê¸°ì‚¬ ì‘ì„±**: ê° ì£¼ì œì— ëŒ€í•´ ê°œë³„ ê¸°ì‚¬ë¥¼ ë‹¨ìˆœíˆ ë‚˜ì—´í•˜ì§€ ë§ê³ , ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **í•˜ë‚˜ì˜ ì™„ê²°ëœ ì‹¬ì¸µ ê¸°ì‚¬**ë¡œ ìƒˆë¡œ ì¨ë¼.
        - **ë¶„ëŸ‰**: ë°˜ë“œì‹œ **ìµœì†Œ 500ì ì´ìƒ**ì˜ ìƒì„¸í•œ ê¸€ë¡œ ì‘ì„±í•  ê²ƒ.
        - **êµ¬ì„±**: ê¸°ì‚¬ì˜ ë°°ê²½, í˜„ì¬ ìƒí™©, ì–¸ë¡ ì‚¬ë³„ ì£¼ìš” ì£¼ì¥, ê·¸ë¦¬ê³  í–¥í›„ ì „ë§ì´ë‚˜ ì „ë¬¸ê°€ ë¶„ì„ ë“± ë‹¤ê°ë„ì˜ ê´€ì ì„ í¬í•¨í•˜ì—¬ ì‘ì„±í•  ê²ƒ.
        - **í†¤**: ì „ë¬¸ê°€ê°€ ì‘ì„±í•œ ê°ê´€ì ì¸ ë…¼ì¡°ì˜ ê¸°ì‚¬ í˜•íƒœë¥¼ ìœ ì§€í•  ê²ƒ.
    3. **ìš”ì•½ë³¸(Bullets)**: ë°”ìœ ë…ìë¥¼ ìœ„í•´, í†µí•© ê¸°ì‚¬ì˜ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ì˜ í•µì‹¬ ë‹¨ë¬¸(Bullet point)ìœ¼ë¡œ ìš”ì•½í•˜ë¼.
    4. ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¥¼ **ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ë”°ë¥´ëŠ” ìœ íš¨í•œ JSON ë¬¸ìì—´ë§Œ** ì¶œë ¥í•˜ë¼.
       - JSON ë°–ì˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸(ì„¤ëª…, ë§ˆí¬ë‹¤ìš´, ì½”ë“œë¸”ë¡ ë“±)ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆë¼.

    [JSON êµ¬ì¡°]
    {{
        "topics": [
            {{
                "title": "ì£¼ì œ ì œëª© (ì˜ˆ: ê¸ˆíˆ¬ì„¸ íì§€ ë…¼ë€ ê°€ì—´)",
                "ids": [0, 2, 5],
                "summary_bullets": ["í•µì‹¬ ë‚´ìš© 1", "í•µì‹¬ ë‚´ìš© 2"],
                "full_article": "ì—¬ê¸°ì— GPTê°€ ìƒˆë¡œ ì‘ì„±í•œ í†µí•© ê¸°ì‚¬ ì „ë¬¸(ì¤„ê¸€ë¡œ ì‘ì„±). 500ì ì´ìƒì„ ì±„ìš°ë„ë¡ ë…¸ë ¥í•´ì•¼ í•œë‹¤."
            }}
        ]
    }}

    [ê¸°ì‚¬ ë°ì´í„°]
    {articles_text}
    """

    response = None
    try:
        # ğŸ”´ ì—¬ê¸°ì—ì„œ ë” ì´ìƒ response_format ì¸ìë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤
        response = client.responses.create(
            model=GPT_MODEL_NAME,
            input=prompt,
        )

        # OpenAI responses êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw_text = ""
        try:
            raw_text = response.output[0].content[0].text.strip()
        except Exception as e:
            print(f"[WARN] response.outputì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨, fallback ì‹œë„: {e}")
            if hasattr(response, "output_text"):
                raw_text = response.output_text.strip()
            else:
                raw_text = str(response).strip()

        # í˜¹ì‹œë¼ë„ ```json ``` ë“± ì½”ë“œë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì ¸ ìˆìœ¼ë©´ ì œê±°
        if raw_text.startswith("```json"):
            raw_text = raw_text.removeprefix("```json").removesuffix("```").strip()
        elif raw_text.startswith("```"):
            raw_text = raw_text.removeprefix("```").removesuffix("```").strip()

        return json.loads(raw_text)

    except json.JSONDecodeError as e:
        print(f"[CRITICAL ERROR] JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        print("--- GPT Raw Output Start ---")
        if response is not None:
            try:
                print(response.output[0].content[0].text)
            except Exception:
                print(str(response))
        else:
            print("No response object available.")
        print("--- GPT Raw Output End ---")
        return {"topics": []}

    except Exception as e:
        print(f"[CRITICAL ERROR] GPT ë¶„ì„ ì¤‘ ê¸°íƒ€ ì—ëŸ¬ ë°œìƒ: {e}")
        return {"topics": []}


# ----------------------------------------
# [Part 4] Telegraph í˜ì´ì§€ ìƒì„± (ì›¹ë·°)
# ----------------------------------------
def create_telegraph_simple(title: str, text_body: str) -> str:
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ Telegraph í˜ì´ì§€ ìƒì„±"""
    try:
        telegraph_account_url = "https://api.telegra.ph/createAccount?short_name=NewsAI"
        print(f"[DEBUG] Telegraph Account URL: {telegraph_account_url}")

        r = requests.get(telegraph_account_url).json()
        token = r["result"]["access_token"]

        content_nodes = []
        content_nodes.append({"tag": "h3", "children": ["AI í†µí•© ë¦¬í¬íŠ¸"]})

        current_p_children = []
        for line in text_body.split("\n"):
            line = line.strip()
            if not line and current_p_children:
                content_nodes.append({"tag": "p", "children": current_p_children})
                current_p_children = []
            elif line:
                current_p_children.append(line)

        if current_p_children:
            content_nodes.append({"tag": "p", "children": current_p_children})

        data = {
            "access_token": token,
            "title": title,
            "content": json.dumps(content_nodes),
            "return_content": False,
        }

        telegraph_create_page_url = "https://api.telegra.ph/createPage"
        resp = requests.post(telegraph_create_page_url, data=data).json()

        if resp.get("ok"):
            return resp["result"]["url"]
        else:
            print(f"Telegraph API ì˜¤ë¥˜: {resp.get('error')}")
            return ""
    except Exception as e:
        print(f"Telegraph ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

# ----------------------------------------
# [Part 5] í…”ë ˆê·¸ë¨ ì „ì†¡ (HTML ëª¨ë“œ)
# ----------------------------------------
def send_telegram(message: str):
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("[WARNING] í…”ë ˆê·¸ë¨ í† í° ë˜ëŠ” ì±„íŒ… IDê°€ ì—†ì–´ ì „ì†¡ì„ ê±´ë„ˆí‚µë‹ˆë‹¤.")
        return

    url = "https://api.telegram.org/bot" + TELEGRAM_BOT_TOKEN + "/sendMessage"

    masked_url = url.replace(TELEGRAM_BOT_TOKEN, "***masked***")
    print(f"[DEBUG] Telegram URL length: {len(url)}")
    print(f"[DEBUG] Telegram URL fragment (masked): {masked_url[:70]}")

    chunk_size = 4000
    for i in range(0, len(message), chunk_size):
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": message[i : i + chunk_size],
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }
        requests.post(url, data=payload)
        time.sleep(0.5)

# ----------------------------------------
# ë©”ì¸ ì‹¤í–‰
# ----------------------------------------
def main():
    # 1. ë§í¬ ìˆ˜ì§‘ ë° í†µê³„
    links = collect_naver_news_links()
    if not links:
        print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    stats = {}
    for item in links:
        stats[item["source"]] = stats.get(item["source"], 0) + 1

    header_stats = " | ".join([f"{k} {v}" for k, v in stats.items()])

    # 2. ë³¸ë¬¸ í¬ë¡¤ë§
    contents = fetch_contents_parallel(links)

    # 3. GPT ë¶„ì„
    if not OPENAI_API_KEY:
        print("OPENAI_API_KEYê°€ ì—†ì–´ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return

    result = analyze_with_gpt(contents)

    # 4. ë¦¬í¬íŠ¸ ë° ì›¹ë·° ì»¨í…ì¸  ìƒì„±
    today_str = get_kst_today()

    telegram_msg = f"<b>ğŸ— {today_str} ì‹ ë¬¸ 1ë©´ ë¸Œë¦¬í•‘</b>\n\n"
    telegram_msg += f"ğŸ“Š <b>ìˆ˜ì§‘ í˜„í™©:</b> {header_stats}\n\n"

    webview_text = f"ğŸ“° {today_str} ì‹ ë¬¸ 1ë©´ í†µí•© ë¦¬í¬íŠ¸\n\n[ìˆ˜ì§‘ í˜„í™©] {header_stats}\n\n"

    topics = result.get("topics", [])

    # ì£¼ì œë³„ ê¸°ì‚¬ ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    topics.sort(key=lambda t: len(t.get("ids", [])), reverse=True)

    if not topics:
        telegram_msg += "<b>âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: ë¶„ì„ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆê±°ë‚˜, AIê°€ ë‹µë³€ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤. GitHub Actions ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.</b>"
        webview_text = "ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨"
    else:
        for topic in topics:
            title = topic.get("title", "ë¬´ì œ")
            ids = topic.get("ids", [])
            bullets = topic.get("summary_bullets", [])
            full_article = topic.get("full_article", "")

            # --- í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ êµ¬ì„± ---
            telegram_msg += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            telegram_msg += f"ğŸ“Œ <b>{title}</b> ({len(ids)}ê±´)\n"

            link_tags = []
            for idx in ids:
                if idx < len(contents):
                    item = contents[idx]
                    link_tags.append(f"<a href='{item['url']}'>{item['source']}</a>")
            telegram_msg += f"ğŸ”— {' , '.join(link_tags)}\n\n"

            for bullet in bullets:
                telegram_msg += f"â€¢ {bullet}\n"
            telegram_msg += "\n"

            # --- ì›¹ë·° í…ìŠ¤íŠ¸ êµ¬ì„± ---
            webview_text += f"\n### ğŸ“Œ {title} ({len(ids)}ê±´)\n"
            webview_text += "\n[í•µì‹¬ ìš”ì•½]\n"
            for bullet in bullets:
                webview_text += f" - {bullet}\n"
            webview_text += "\n[í†µí•© ì‹¬ì¸µ ê¸°ì‚¬]\n"
            webview_text += f"{full_article}\n"
            webview_text += "\n\n"

    # 5. Telegraph í˜ì´ì§€ ìƒì„± (ê¸´ í™”ë©´ìš©)
    webview_url = create_telegraph_simple(f"{today_str} ì¡°ê°„ ë¸Œë¦¬í•‘", webview_text)

    if webview_url:
        telegram_msg += f"\n\nğŸ“± <b><a href='{webview_url}'>ğŸ‘‰ ì „ì²´ ë¦¬í¬íŠ¸ í¬ê²Œ ë³´ê¸° (Safari/Web)</a></b>"

    # 6. ì „ì†¡
    print("[INFO] í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘...")
    send_telegram(telegram_msg)
    print("[INFO] ì™„ë£Œ.")

if __name__ == "__main__":
    main()
