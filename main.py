import os
import time
import json
import html
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from typing import List, Tuple, Dict
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# [NEW] Google Generative AI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import google.generativeai as genai
from google.api_core import retry

# ----------------------------------------
# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
# ----------------------------------------
load_dotenv()

def get_gemini_api_key() -> str:
    """
    GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì½ì–´ì„œ ê³µë°± ì œê±° í›„ ë¦¬í„´.
    """
    key = os.getenv("GEMINI_API_KEY", "")
    return key.strip()

GEMINI_API_KEY = get_gemini_api_key()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").strip()

if not GEMINI_API_KEY:
    raise SystemExit(
        "[ERROR] GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"
        " - .env íŒŒì¼ì— GEMINI_API_KEY=... ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\n"
        " - Google AI Studio(https://aistudio.google.com/)ì—ì„œ í‚¤ë¥¼ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

# [NEW] Gemini ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)

# ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: gemini-1.5-flash)
# ë‰´ìŠ¤ ìš”ì•½ìš©ìœ¼ë¡œëŠ” 1.5 Flashê°€ ì†ë„/ë¹„ìš© ë©´ì—ì„œ ìœ ë¦¬í•˜ë©°,
# ë” ê¹Šì€ ì¶”ë¡ ì´ í•„ìš”í•˜ë©´ 'gemini-1.5-pro'ë¡œ ë³€ê²½í•˜ì„¸ìš”.
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-1.5-flash").strip()

def escape_html(text: str) -> str:
    """Escape user/content strings for safe Telegram HTML."""
    return html.escape(text or "", quote=True)

PRESS_LIST: List[Tuple[str, str]] = [
    ("ë™ì•„ì¼ë³´", "020"),
    ("í•œêµ­ì¼ë³´", "469"),
    ("ì¡°ì„ ì¼ë³´", "023"),
    ("ì¤‘ì•™ì¼ë³´", "025"),
    ("í•œê²¨ë ˆ", "028"),
    ("ê²½í–¥ì‹ ë¬¸", "032"),
]

# ----------------------------------------
# [Part 1] ë„¤ì´ë²„ 1ë©´ ë§í¬ ìˆ˜ì§‘ (ê¸°ì¡´ ë™ì¼)
# ----------------------------------------
def get_kst_today() -> str:
    now_utc = datetime.now(timezone.utc)
    now_kst = now_utc + timedelta(hours=9)
    return now_kst.strftime("%Y%m%d")

def fetch_html(url: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    resp = requests.get(url.strip(), headers=headers, timeout=20)
    resp.raise_for_status()
    return resp.text

def extract_a1_links(html: str, page_url: str, press_code: str, date: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates: List[str] = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if f"/article/newspaper/{press_code}/" not in href:
            continue
        if f"date={date}" not in href:
            continue
        full_url = urljoin(page_url, href)

        is_a1 = False
        parent = a
        for _ in range(6):
            parent = parent.parent
            if parent is None:
                break
            text = parent.get_text(" ", strip=True)
            if any(key in text for key in ["A1ë©´", "A01ë©´", "1ë©´", "1 é¢"]):
                is_a1 = True
                break
        if is_a1:
            candidates.append(full_url)

    if not candidates:  # Fallback
        seen = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if f"/article/newspaper/{press_code}/" in href and f"date={date}" in href:
                full_url = urljoin(page_url, href)
                if full_url not in seen:
                    candidates.append(full_url)
                    seen.add(full_url)
            if len(candidates) >= 4:
                break
    return list(set(candidates))

def collect_naver_news_links() -> List[Dict[str, str]]:
    date = get_kst_today()
    print(f"[INFO] {date}ì¼ì 1ë©´ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘")
    all_items = []
    for press_name, press_code in PRESS_LIST:
        url = ""
        try:
            url = f"https://media.naver.com/press/{press_code}/newspaper?date={date}".strip()
            html = fetch_html(url)
            links = extract_a1_links(html, url, press_code, date)
            for link in links:
                all_items.append({"source": press_name, "url": link})
        except Exception as e:
            print(f"  [ì—ëŸ¬] {press_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    return all_items

# ----------------------------------------
# [Part 2] ë³¸ë¬¸ í¬ë¡¤ë§ (ê¸°ì¡´ ë™ì¼)
# ----------------------------------------
def fetch_single_article_content(item: dict) -> dict:
    try:
        resp = requests.get(item["url"], headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        selectors = ["div#dic_area", "div#newsEndContents", "div.newsct_article", "div#articleBodyContents"]
        content = ""
        for selector in selectors:
            node = soup.select_one(selector)
            if node:
                content = node.get_text("\n", strip=True)
                break
        return {
            "source": item["source"],
            "url": item["url"],
            "content": content[:4000] if content else "ë³¸ë¬¸ ì—†ìŒ"
        }
    except Exception:
        return item

def fetch_contents_parallel(items: list) -> list:
    print(f"[INFO] ì´ {len(items)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_single_article_content, items))
    return results

# ----------------------------------------
# [Part 3] Gemini ë¶„ì„ (ë¦¬í¬íŠ¸ ì‘ì„±) - ë³€ê²½ë¨
# ----------------------------------------
def analyze_with_gemini(articles: list) -> dict:
    print(f"[INFO] {GEMINI_MODEL_NAME} ë¶„ì„ ìš”ì²­ ì‹œì‘...")

    # ê¸°ì‚¬ ë³¸ë¬¸ ëª¨ìœ¼ê¸°
    articles_text = ""
    for i, art in enumerate(articles):
        articles_text += f"[ID:{i}] ì–¸ë¡ ì‚¬:{art['source']} | ë‚´ìš©:{art['content'][:2000]}\n"

    # Geminiì—ê²Œ ìš”ì²­í•  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_instruction = """
    ë„ˆëŠ” ì „ë¬¸ ë‰´ìŠ¤ ì—ë””í„°ë‹¤. ì˜¤ëŠ˜ì ì‹ ë¬¸ 1ë©´ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ê³ í’ˆì§ˆ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ë¼.
    
    [ìš”êµ¬ì‚¬í•­]
    1. ê¸°ì‚¬ë“¤ì„ ìœ ì‚¬í•œ ì£¼ì œ(ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“±)ë¡œ ê·¸ë£¹í™”í•˜ë¼.
    2. ì£¼ì œë³„ í†µí•© ê¸°ì‚¬ ì‘ì„±: ê° ì£¼ì œì— ëŒ€í•´ ê°œë³„ ê¸°ì‚¬ë¥¼ ë‹¨ìˆœíˆ ë‚˜ì—´í•˜ì§€ ë§ê³ , ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ê²°ëœ ì‹¬ì¸µ ê¸°ì‚¬ë¡œ ìƒˆë¡œ ì¨ë¼.
       - ë¶„ëŸ‰: ìµœì†Œ 500ì ì´ìƒ.
       - êµ¬ì„±: ë°°ê²½, í˜„í™©, ì–¸ë¡ ì‚¬ë³„ ì£¼ìš” ì£¼ì¥, ì „ë§ ë“±ì„ í¬í•¨.
       - í†¤: ê°ê´€ì ì¸ ë…¼ì¡° ìœ ì§€.
    3. ìš”ì•½ë³¸(Bullets): ë°”ìœ ë…ìë¥¼ ìœ„í•´ 3ì¤„ ì´ë‚´ í•µì‹¬ ìš”ì•½.
    4. ì–¸ë¡ ì‚¬ë³„ ë¹„íŒ/ë…¼ì¡° ì •ë¦¬: í•´ë‹¹ ì£¼ì œ ë‚´ ê¸°ì‚¬ë“¤ì˜ ì–¸ë¡ ì‚¬ë³„ ë…¼ì¡°(ë¹„íŒ, ì˜¹í˜¸, ìš°ë ¤ ë“±)ë¥¼ ìš”ì•½.
    
    ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•´ì•¼ í•œë‹¤.
    """

    # Gemini 1.5ë¶€í„°ëŠ” JSON ìŠ¤í‚¤ë§ˆë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆìœ¼ë‚˜, 
    # ì—¬ê¸°ì„œëŠ” í”„ë¡¬í”„íŠ¸ ë‚´ ì˜ˆì‹œì™€ response_mime_type ì„¤ì •ì„ í†µí•´ ì œì–´í•©ë‹ˆë‹¤.
    prompt = f"""
    [ê¸°ì‚¬ ë°ì´í„°]
    {articles_text}

    [ì¶œë ¥ JSON í˜•ì‹ì„ ì—„ìˆ˜í•  ê²ƒ]
    {{
        "topics": [
            {{
                "title": "ì£¼ì œ ì œëª©",
                "ids": [0, 2],
                "summary_bullets": ["ìš”ì•½1", "ìš”ì•½2"],
                "full_article": "í†µí•© ì¤„ê¸€ ê¸°ì‚¬ (500ì ì´ìƒ)",
                "press_critiques": [
                    {{
                        "source": "ì–¸ë¡ ì‚¬ëª…",
                        "position": "ë…¼ì¡° ë° ì£¼ì¥ ìš”ì•½",
                        "tone": "ë¹„íŒì /ì˜¹í˜¸ì /ì¤‘ë¦½ì "
                    }}
                ]
            }}
        ]
    }}
    """

    try:
        # ëª¨ë¸ ì„¤ì • (JSON ëª¨ë“œ í™œì„±í™”)
        model = genai.GenerativeModel(
            model_name=GEMINI_MODEL_NAME,
            system_instruction=system_instruction,
            generation_config={
                "response_mime_type": "application/json",
                "temperature": 0.3, # ë‰´ìŠ¤ ë¶„ì„ì´ë¯€ë¡œ ì°½ì˜ì„±ë³´ë‹¤ëŠ” ì •í™•ì„± ì¤‘ìš”
            }
        )
        
        # API ìš”ì²­ (Retry ì •ì±… ì ìš© ê¶Œì¥)
        response = model.generate_content(prompt, request_options={"retry": retry.Retry(predicate=retry.if_transient_error)})
        
        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° JSON íŒŒì‹±
        raw_text = response.text
        return json.loads(raw_text)

    except json.JSONDecodeError as e:
        print(f"[CRITICAL ERROR] JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        # print(raw_text) 
        return {"topics": []}

    except Exception as e:
        print(f"[CRITICAL ERROR] Gemini ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return {"topics": []}


# ----------------------------------------
# [Part 4] Telegraph í˜ì´ì§€ ìƒì„± (ê¸°ì¡´ ë™ì¼)
# ----------------------------------------
def create_telegraph_simple(title: str, text_body: str) -> str:
    try:
        telegraph_account_url = "https://api.telegra.ph/createAccount?short_name=NewsAI"
        r = requests.get(telegraph_account_url, timeout=10).json()
        token = r["result"]["access_token"]

        content_nodes = []
        content_nodes.append({"tag": "h3", "children": ["AI í†µí•© ë¦¬í¬íŠ¸"]})

        for raw_line in text_body.split("\n"):
            line = raw_line.strip()
            if not line:
                continue 

            if line.startswith("### "):
                content_nodes.append({
                    "tag": "h4",
                    "children": [line[4:]]
                })
            elif line.startswith("[") and line.endswith("]"):
                content_nodes.append({
                    "tag": "p",
                    "children": [{
                        "tag": "b",
                        "children": [line]
                    }]
                })
            else:
                content_nodes.append({
                    "tag": "p",
                    "children": [line]
                })

        data = {
            "access_token": token,
            "title": title,
            "content": json.dumps(content_nodes),
            "return_content": False,
        }

        telegraph_create_page_url = "https://api.telegra.ph/createPage"
        resp = requests.post(telegraph_create_page_url, data=data, timeout=10).json()

        if resp.get("ok"):
            return resp["result"]["url"]
        else:
            print(f"Telegraph API ì˜¤ë¥˜: {resp.get('error')}")
            return ""
    except Exception as e:
        print(f"Telegraph ìƒì„± ì‹¤íŒ¨: {e}")
        return ""


# ----------------------------------------
# [Part 5] í…”ë ˆê·¸ë¨ ì „ì†¡ (ê¸°ì¡´ ë™ì¼)
# ----------------------------------------
def send_telegram(message: str):
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("[WARNING] í…”ë ˆê·¸ë¨ í† í° ì„¤ì • ëˆ„ë½. ì „ì†¡ ìƒëµ.")
        return

    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    def split_message(msg: str, chunk_size: int = 4000) -> list[str]:
        chunks = []
        current = []
        current_len = 0
        for line in msg.splitlines(keepends=True):
            if len(line) >= chunk_size:
                if current:
                    chunks.append("".join(current))
                    current = []
                    current_len = 0
                for i in range(0, len(line), chunk_size):
                    chunks.append(line[i : i + chunk_size])
                continue
            if current_len + len(line) > chunk_size:
                chunks.append("".join(current))
                current = [line]
                current_len = len(line)
            else:
                current.append(line)
                current_len += len(line)
        if current:
            chunks.append("".join(current))
        return chunks

    chunks = split_message(message, chunk_size=4000)

    for i, chunk_text in enumerate(chunks):
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": chunk_text,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }
        try:
            resp = requests.post(url, data=payload, timeout=10)
            if resp.status_code != 200:
                print(f"[ERROR] í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨ ({i}): {resp.text}")
            time.sleep(0.5)
        except Exception as e:
            print(f"[ERROR] í…”ë ˆê·¸ë¨ ìš”ì²­ ì¤‘ ì˜ˆì™¸: {e}")

    print("[INFO] í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")


# ----------------------------------------
# ë©”ì¸ ì‹¤í–‰
# ----------------------------------------
def main():
    # 1. ë§í¬ ìˆ˜ì§‘
    links = collect_naver_news_links()
    if not links:
        print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    stats = {}
    for item in links:
        stats[item["source"]] = stats.get(item["source"], 0) + 1
    header_stats = " | ".join([f"{k} {v}" for k, v in stats.items()])
    safe_header_stats = escape_html(header_stats)

    # 2. ë³¸ë¬¸ í¬ë¡¤ë§
    contents = fetch_contents_parallel(links)

    # 3. Gemini ë¶„ì„
    if not GEMINI_API_KEY:
        print("GEMINI_API_KEYê°€ ì—†ì–´ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return

    result = analyze_with_gemini(contents)

    # 4. ë¦¬í¬íŠ¸ ìƒì„±
    today_str = get_kst_today()
    telegram_msg = f"<b>ğŸ— {today_str} ì‹ ë¬¸ 1ë©´ ë¸Œë¦¬í•‘ (Powered by Gemini)</b>\n\n"
    telegram_msg += f"ğŸ“Š <b>ìˆ˜ì§‘ í˜„í™©:</b> {safe_header_stats}\n\n"
    webview_text = f"ğŸ“° {today_str} ì‹ ë¬¸ 1ë©´ í†µí•© ë¦¬í¬íŠ¸\n\n[ìˆ˜ì§‘ í˜„í™©] {header_stats}\n\n"

    topics = result.get("topics", [])
    topics.sort(key=lambda t: len(t.get("ids", [])), reverse=True)

    if not topics:
        telegram_msg += "<b>âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</b>"
        webview_text = "ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨"
    else:
        for topic in topics:
            title = topic.get("title", "ë¬´ì œ")
            ids = topic.get("ids", [])
            bullets = topic.get("summary_bullets", [])
            full_article = topic.get("full_article", "")
            press_critiques = topic.get("press_critiques", [])

            # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€
            telegram_msg += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            telegram_msg += f"ğŸ“Œ <b>{escape_html(title)}</b> ({len(ids)}ê±´)\n"
            
            link_tags = []
            for idx in ids:
                if idx < len(contents):
                    item = contents[idx]
                    link_tags.append(
                        f"<a href=\"{escape_html(item['url'])}\">{escape_html(item['source'])}</a>"
                    )
            telegram_msg += f"ğŸ”— {' , '.join(link_tags)}\n\n"

            for bullet in bullets:
                telegram_msg += f"â€¢ {escape_html(bullet)}\n"
            telegram_msg += "\n"

            if press_critiques:
                telegram_msg += "ğŸ“° <b>ì–¸ë¡ ì‚¬ë³„ ë…¼ì¡°</b>\n"
                for pc in press_critiques:
                    src = pc.get("source", "")
                    pos = pc.get("position", "")
                    if src and pos:
                        telegram_msg += f"- {escape_html(src)}: {escape_html(pos)}\n"
                telegram_msg += "\n"

            # ì›¹ë·° í…ìŠ¤íŠ¸
            webview_text += f"\n### ğŸ“Œ {title} ({len(ids)}ê±´)\n"
            webview_text += "\n[í•µì‹¬ ìš”ì•½]\n"
            for bullet in bullets:
                webview_text += f" - {bullet}\n"
            
            webview_text += "\n[í†µí•© ì‹¬ì¸µ ê¸°ì‚¬]\n"
            webview_text += f"{full_article}\n"

            if press_critiques:
                webview_text += "\n[ì–¸ë¡ ì‚¬ë³„ ë¹„íŒ/ë…¼ì¡°]\n"
                for pc in press_critiques:
                    src = pc.get("source", "")
                    pos = pc.get("position", "")
                    tone = pc.get("tone", "")
                    webview_text += f" - {src}: ({tone}) {pos}\n"
            webview_text += "\n\n"

    # 5. Telegraph ë§í¬ ìƒì„± ë° ì „ì†¡
    webview_url = create_telegraph_simple(f"{today_str} ì¡°ê°„ ë¸Œë¦¬í•‘", webview_text)

    if webview_url:
        telegram_msg += f"\n\nğŸ“± <b><a href='{webview_url}'>ğŸ‘‰ ì „ì²´ ë¦¬í¬íŠ¸ í¬ê²Œ ë³´ê¸°</a></b>"
    
    send_telegram(telegram_msg)

if __name__ == "__main__":
    main()
