import os
import time
import json
import requests
from datetime import datetime, timedelta
from urllib.parse import urljoin
from typing import List, Tuple, Dict, Optional
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import google.generativeai as genai
from dotenv import load_dotenv

# ----------------------------------------
# í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
# ----------------------------------------
load_dotenv() # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (.env íŒŒì¼ ë¡œë“œ)

# GitHub Actionsì—ì„œëŠ” Secretsì—ì„œ ì£¼ì…ë¨
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

if not GEMINI_API_KEY:
    print("[ê²½ê³ ] GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. (ë¡œì»¬ í…ŒìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ë©´ GitHub Secrets í™•ì¸ í•„ìš”)")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Naver ì‹ ë¬¸ì‚¬ë³„ ì½”ë“œ (ì œê³µí•´ì£¼ì‹  ì½”ë“œ í™œìš©)
PRESS_LIST: List[Tuple[str, str]] = [
    ("ë™ì•„ì¼ë³´", "020"),
    ("í•œêµ­ì¼ë³´", "469"),
    ("ì¡°ì„ ì¼ë³´", "023"),
    ("ì¤‘ì•™ì¼ë³´", "025"),
    ("í•œê²¨ë ˆ", "028"),
    ("ê²½í–¥ì‹ ë¬¸", "032"),
]
BASE_NEWPAPER_URL = "https://media.naver.com/press/{press}/newspaper?date={date}"


# ----------------------------------------
# [Part 1] ë„¤ì´ë²„ 1ë©´ ë§í¬ ìˆ˜ì§‘ (Crawler)
# ----------------------------------------
def get_kst_today() -> str:
    """í˜„ì¬ KST(UTC+9) ê¸°ì¤€ ë‚ ì§œë¥¼ YYYYMMDDë¡œ ë°˜í™˜"""
    now_utc = datetime.utcnow()
    now_kst = now_utc + timedelta(hours=9)
    return now_kst.strftime("%Y%m%d")

def fetch_html(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    }
    resp = requests.get(url, headers=headers, timeout=20)
    resp.raise_for_status()
    return resp.text

def extract_a1_links(html: str, page_url: str, press_code: str, date: str) -> List[str]:
    """A1(1ë©´) ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, "html.parser")
    candidates: List[str] = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if f"/article/newspaper/{press_code}/" not in href: continue
        if f"date={date}" not in href: continue

        full_url = urljoin(page_url, href)

        # ë¶€ëª¨ ìª½ì— A1/1ë©´ í‘œì‹œ ìˆëŠ”ì§€ í™•ì¸
        is_a1 = False
        parent = a
        for _ in range(6):
            parent = parent.parent
            if parent is None: break
            text = parent.get_text(" ", strip=True)
            if any(key in text for key in ["A1ë©´", "A01ë©´", "A 1ë©´", "A 01ë©´", "1ë©´", "1 é¢"]):
                is_a1 = True
                break

        if is_a1:
            candidates.append(full_url)

    # Fallback: A1 í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ìƒìœ„ 4ê°œ ê°€ì ¸ì˜¤ê¸°
    if not candidates:
        seen = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if f"/article/newspaper/{press_code}/" in href and f"date={date}" in href:
                full_url = urljoin(page_url, href)
                if full_url not in seen:
                    candidates.append(full_url)
                    seen.add(full_url)
            if len(candidates) >= 4:
                break
    
    # ì¤‘ë³µ ì œê±°
    return list(set(candidates))

def collect_naver_news_links() -> List[Dict[str, str]]:
    """ëª¨ë“  ì–¸ë¡ ì‚¬ì˜ 1ë©´ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    date = get_kst_today()
    print(f"[INFO] {date}ì¼ì 1ë©´ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘")
    
    all_items = []
    
    for press_name, press_code in PRESS_LIST:
        page_url = BASE_NEWPAPER_URL.format(press=press_code, date=date)
        try:
            html = fetch_html(page_url)
            links = extract_a1_links(html, page_url, press_code, date)
            print(f"  - {press_name}: {len(links)}ê°œ ë°œê²¬")
            for link in links:
                all_items.append({"source": press_name, "url": link})
        except Exception as e:
            print(f"  [ì—ëŸ¬] {press_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            
    return all_items

# ----------------------------------------
# [Part 2] ë³¸ë¬¸ í¬ë¡¤ë§ (Parallel Fetcher)
# ----------------------------------------
def fetch_single_article_content(item: dict) -> dict:
    """ë‹¨ì¼ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    url = item["url"]
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì…€ë ‰í„° ëª¨ìŒ
        selectors = [
            "div#dic_area", "div#newsEndContents", "div.newsct_article",
            "div#articeBody", "div#articleBodyContents"
        ]
        content = ""
        for selector in selectors:
            node = soup.select_one(selector)
            if node:
                content = node.get_text("\n", strip=True)
                break
        
        return {
            "source": item["source"],
            "url": url,
            "content": content if content else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"
        }
    except Exception as e:
        return {"source": item["source"], "url": url, "content": f"ì—ëŸ¬: {e}"}

def fetch_contents_parallel(items: list) -> list:
    """ThreadPoolë¡œ ë¹ ë¥´ê²Œ ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸°"""
    print(f"[INFO] ì´ {len(items)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(fetch_single_article_content, items))
    return results

# ----------------------------------------
# [Part 3] Gemini ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
# ----------------------------------------
def analyze_with_gemini(articles: list) -> dict:
    print("[INFO] Gemini 1.5 Flash ë¶„ì„ ìš”ì²­ ì‹œì‘...")
    
    # ëª¨ë¸ëª… ìˆ˜ì •: 'gemini-1.5-flash-latest' -> 'gemini-1.5-flash'
    model = genai.GenerativeModel(
        model_name='gemini-1.5-flash', 
        generation_config={"response_mime_type": "application/json"}
    )

    articles_text = ""
    for i, art in enumerate(articles):
        articles_text += f"[ID:{i}] {art['source']} - {art['content'][:3000]}\n" # ë„ˆë¬´ ê¸¸ë©´ ìë¦„

    prompt = f"""
    ì˜¤ëŠ˜ì í•œêµ­ ì£¼ìš” ì‹ ë¬¸ 1ë©´ ê¸°ì‚¬ë“¤ì´ë‹¤. 
    ì´ ë‚´ìš©ë“¤ì„ ì¢…í•©í•´ 'ì˜¤ëŠ˜ì˜ ì¡°ê°„ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ë¼.

    [ìš”êµ¬ì‚¬í•­]
    1. ì „ì²´ë¥¼ ê´€í†µí•˜ëŠ” í•µì‹¬ ì´ìŠˆì™€ ë¶„ìœ„ê¸° ìš”ì•½ (Markdown í˜•ì‹)
    2. ì£¼ìš” ì£¼ì œë³„(ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“±)ë¡œ ê¸°ì‚¬ë“¤ì„ ë¶„ë¥˜í•˜ê³  ê° ì£¼ì œì— ëŒ€í•´ ê° ì–¸ë¡ ì‚¬ì˜ ë…¼ì¡°(Tone)ë¥¼ ë¹„êµ ë¶„ì„í•˜ë¼.
    3. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ë¼.

    [JSON ì¶œë ¥ í˜•ì‹]
    {{
        "report_body": "ì—¬ê¸°ì— ì „ì²´ ë¦¬í¬íŠ¸ ë³¸ë¬¸(ë§ˆí¬ë‹¤ìš´) ì‘ì„±. ì´ëª¨ì§€ ì‚¬ìš©í•´ì„œ ê°€ë…ì„± ë†’ì¼ ê²ƒ.",
        "topics": [
            {{ "title": "ì£¼ì œA", "ids": [0, 1, 5] }},
            {{ "title": "ì£¼ì œB", "ids": [2, 3] }}
        ]
    }}

    [ê¸°ì‚¬ ëª©ë¡]
    {articles_text}
    """

    try:
        response = model.generate_content(prompt)
        return json.loads(response.text)
    except Exception as e:
        print(f"[ì—ëŸ¬] Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {"report_body": "ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "topics": []}

# ----------------------------------------
# [Part 4] í…”ë ˆê·¸ë¨ ì „ì†¡
# ----------------------------------------
def send_telegram(message: str):
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        return

    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    chunk_size = 3500 # í…”ë ˆê·¸ë¨ ì œí•œ ëŒ€ë¹„ ì—¬ìœ ìˆê²Œ

    for i in range(0, len(message), chunk_size):
        data = {"chat_id": TELEGRAM_CHAT_ID, "text": message[i:i+chunk_size], "parse_mode": "Markdown"}
        requests.post(url, data=data)
        time.sleep(0.5)

# ----------------------------------------
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ----------------------------------------
def main():
    # 1. ë§í¬ ìˆ˜ì§‘
    links = collect_naver_news_links()
    if not links:
        print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. ë³¸ë¬¸ í¬ë¡¤ë§
    contents = fetch_contents_parallel(links)

    # 3. Gemini ë¶„ì„
    if not GEMINI_API_KEY:
        print("API í‚¤ê°€ ì—†ì–´ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return
    
    result = analyze_with_gemini(contents)
    
    # 4. ë¦¬í¬íŠ¸ ì¡°ë¦½
    final_report = f"ğŸ— *ì˜¤ëŠ˜ì˜ ì‹ ë¬¸ 1ë©´ ë¸Œë¦¬í•‘* ({get_kst_today()})\n\n"
    final_report += result.get("report_body", "")
    
    final_report += "\n\nğŸ”— *ê´€ë ¨ ê¸°ì‚¬ ì›ë¬¸*\n"
    for topic in result.get("topics", []):
        final_report += f"\nğŸ“Œ *{topic['title']}*\n"
        
        # í•´ë‹¹ ì£¼ì œì˜ ê¸°ì‚¬ë“¤ ëª¨ìœ¼ê¸°
        topic_urls = {}
        for idx in topic['ids']:
            if idx < len(contents):
                item = contents[idx]
                src = item['source']
                if src not in topic_urls: topic_urls[src] = []
                topic_urls[src].append(item['url'])
        
        for src, urls in topic_urls.items():
            # ë§í¬ê°€ ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ë§Œ ëŒ€í‘œë¡œ í‘œì‹œí•˜ê±°ë‚˜ ë‚˜ì—´
            final_report += f"- {src}: [ê¸°ì‚¬ë³´ê¸°]({urls[0]})\n"

    # 5. ì „ì†¡
    print("[INFO] í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘...")
    send_telegram(final_report)
    print("[INFO] ì™„ë£Œ.")

if __name__ == "__main__":
    main()
